# Knowledge-Distillation
Knowledge Distillation is a procedure for model compression, in which a small (student) model is trained to match a large pre-trained (teacher) model. Knowledge is transferred from the teacher model to the student by minimizing a loss function, aimed at matching softened teacher logits as well as ground-truth labels


Code is in `PyTorch`. Inspired by [this](https://keras.io/examples/vision/knowledge_distillation/)
